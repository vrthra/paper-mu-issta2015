\newcommand*\per{\scalebox{.5}{\%}}
\newcommand\Mua{Mutation analysis~}
\newcommand\mua{mutation analysis~}
\begin{comment}
<<setup, include=FALSE, cache=FALSE, message=FALSE, echo=FALSE>>=
#options(warn=-1)
options(digits=3)
opts_chunk$set(fig.path='figure/twocolumn-', fig.align='center', fig.show='hold', echo=FALSE)
render_listings()
logit<-function(x) log(x/(1-x))
@
<<chunk-hook, echo=2>>=
<<setup>>
@
\end{comment}


<<a,message=F, echo=F>>=
require(xtable)
require(data.table)
load('data/mu.rda')
@

% 10 pages for the main text + 1 for references.
\section{Introduction}
% 1. Describe the problem
% 2. State your contributions

Mutation analysis originally proposed by Lipton\cite{lipton1971fault} involves systematic transformation of program through introduction of first order syntactical changes\cite{ammann2008introduction}. Researchers have shown that the mutations thus introduced behave similarly to the real faults\cite{andrews2005mutation}, and hence can be used as a stand-in for real faults when determining the efficacy of a test suite against faults.

While mutation analysis provides a useful metric, its use has been held back due to its high computational requirements\cite{jia2011analysis}. This has led researchers to search for ways of reducing the resource requirements of the analysis.
The methods in this direction can be classified into ``Do faster'', ``Do fewer'', and ``Do smarter''\cite{offutt2001mutation}. The ``Do fewer'' approach has come in for special study, and various researchers have investigated different ways to reduce the number of mutants thus produced.

One of the main approaches is selective mutation, proposed by Mathur\cite{mathur1991performance}, and further refined by Offutt et al.\cite{offutt1993anexperimental}. This approach relies on the fact that some of the mutagens generate a large number of variants, and hence produce highly redundant test cases. Previous research suggested that elimination of mutagens with high fertility results in the reduction of total number of mutations produced, and hence reduces the computational requirements.

% TODO check the survey again on selective mutation.

It was recently questioned whether this approach indeed resulted in a tangible improvement over random selection of mutants\cite{zhang2010operator}, which did not find operator based selection to be better than random selection. It was later found that using operator based selection and random sampling was better than using each strategy separately\cite{zhang2013ase}.

Another research, initially by Unch\cite{untch2009onreduced} and recently by Deng et al.\cite{deng2013empirical} suggests using software deletion operator since it produces the least number of mutants (smallest neighborhood). The researchers were able to obtain a close approximation of original mutant score with significant savings with this approach.

However, our previous research\cite{icse} suggests that mutation coverage is closely predicted by statement coverage, this suggests that any criteria that forces accounting of the impact of each statement should provide a close approximation to the full mutation score. Hence, our research suggests that SDL works because it forces accounting of each line, and a random sampling criteria that provides the same criteria should result in similar results.

In our analysis, we find that a random sampling approach that randomly chose a single mutant per line was found to have a better correlation with the full mutation score than that obtained by SDL. Further it was also found that the mutation score produced by random sampling was closer to the full mutation score than that produced by SDL.

\section{Related Work}
Related work.
\section{Methodology}
Our methodology

\section{Analysis}

<<prepare,echo=F>>=
attach(m)
sdl_f <- mut.avg~mut.avg.sdl + 0
rnd_f <- mut.avg~mut.avg.rnd + 0

sdl_all <- lm(sdl_f)
rnd_all <- lm(rnd_f)

sdl_s <- summary(sdl_all)
rnd_s <- summary(rnd_all)

avg.est <- sdl_s$coefficients['mut.avg.sdl','Estimate']
rnd.est <- rnd_s$coefficients['mut.avg.rnd','Estimate']
@

\begin{figure*}[t]
<<plots, fig.width=5, fig.height=5, out.width='.45\\linewidth', out.heigth='4in', echo=F, message=F, warn=F,fig.lp="fig:">>=
plot(sdl_f, main='SDL ~ ALL',pch='*')
abline(sdl_all, col='red')
plot(rnd_f, main='RND ~ ALL',pch='*')
abline(rnd_all, col='blue')
abline(sdl_all, col='red', lty=2)
@
\caption{Plots}
\label{fig:caddrm}
\end{figure*}

\subsection{Correlation Analysis}

$ \mu_{all} = 0 + \beta_0 \times \mu_{sample}  $

The interesting part is (1) the $\beta_0$ coeff which tells how much we must multiply the  sampled/selected score to get original score, and $R^2$ which tells us how close the score   is.

\subsubsection{mut.avg.sdl and mut.avg}

$ \mu_{all} = 0 + \Sexpr{avg.est} \times \mu_{sdl}  $

\subsubsection{mut.avg.rnd and mut.avg}

$ \mu_{all} = 0 + \Sexpr{rnd.est} \times \mu_{rnd}  $

\section{Results}

We see from the data that random sampling has a higher $R^2$, and also approximates the all  mutations score better.
However, caveat emptor. There is a difference between the number of mutators for SDL and RAND. If our approximation was right, we should have gotten nearly same number of mutators. However
The number of mutators in RND is \Sexpr{sum(m$pit.mutation.total.rnd) / sum(m$pit.mutation.total.sdl)} times SDL.
