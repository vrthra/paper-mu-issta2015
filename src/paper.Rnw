\newcommand*\per{\scalebox{.5}{\%}}
\newcommand\Mua{Mutation analysis~}
\newcommand\mua{mutation analysis~}
\begin{comment}
<<setup, include=FALSE, cache=FALSE, message=FALSE, echo=FALSE>>=
clean <- function(x)sub('CONSTRUCTOR','CONSTR',sub('RETURN','R',sub('INCREMENTS','INC',sub('VOID','V',sub('CONDITIONAL','COND',sub('NON','N',sub('VARIABLE','VAR',sub('MEMBER','MEM',sub('DELETE_','D_',sub('REMOVE_','R_',sub('NEGATE_','N_',sub('_CALLS','',sub('EXPERIMENTAL_','E_',x)))))))))))))
#options(warn=-1)
options(digits=3)
opts_chunk$set(fig.path='figure/twocolumn-', fig.align='center', fig.show='hold', echo=FALSE)
render_listings()
logit<-function(x) log(x/(1-x))
require(reshape2)
require(xtable)
require(Mutsel)
@
<<chunk-hook, echo=2>>=
<<setup>>
v=data(package='Mutsel')
data(list = v$results[,3])
@
\end{comment}


% <<a,message=F, echo=F>>=
% require(xtable)
% require(data.table)
% load('data/mu.rda')
% @

% 10 pages for the main text + 1 for references.
<<>>=
data(o.dist.all)
data(o.dist.det)
data(r.dist.all)
data(r.dist.det)
@
<<intro, child='intro.Rnw'>>=
@
<<related, child='related.Rnw'>>=
@
<<methodology, child='methodology.Rnw'>>=
@
<<analysis, child='analysis.Rnw'>>=
@


\subsection{Occurrence of Operators}
\begin{figure*}[t]
<<op,fig.width=10, fig.height=5, out.width='.95\\linewidth', out.heigth='4in', echo=F, message=F, warn=F,fig.lp="fig:">>=
par(las=1,mfrow = c(1, 2),mar=c(2,8,1,1))
o.all <- cbind(projects = rownames(o.dist.all), stack(o.dist.all))
o.all$ind <- clean(o.all$ind)
bcol <- '#2171B5'
#bcol <- 'darkgray'
icol <- '#9ECAE0'
#icol <- 'lightgray'
o.det <- cbind(projects = rownames(o.dist.det), stack(o.dist.det))
o.det$ind <- clean(o.det$ind)
with(o.det,boxplot(values~ind,horizontal=T,border=c(bcol), col=icol, pch='.', axes=F, ylim=c(0,0.6)))
with(o.all,boxplot(values~ind,horizontal=T,border=c('black'), pch='.',main='Original', cex.axis=0.8, ylim=c(0,0.6),add=T))

r.all <- cbind(projects = rownames(r.dist.all), stack(r.dist.all))
r.all$ind <- clean(r.all$ind)
r.det <- cbind(projects = rownames(r.dist.det), stack(r.dist.det))
r.det$ind <- clean(r.det$ind)
with(r.det,boxplot(values~ind,horizontal=T,border=c(bcol), col=icol, pch='.', axes=F,ylim=c(0,0.6)))
with(r.all,boxplot(values~ind,horizontal=T,border=c('black'), pch='.',main='Randoop', cex.axis=0.8,ylim=c(0,0.6),add=T))
@
\caption{Relative fertility of mutation operators. The blue shade represent detected mutants, while black represents total mutants.}
\label{fig:op}
\end{figure*}

\subsection{SDL vs Random selection}
\begin{figure*}[t]
<<sdl.1,fig.width=10, fig.height=10, out.width='.95\\linewidth', out.heigth='4in', echo=F, message=F, warn=F,fig.lp="fig:">>=
par(mfrow=c(2,2))
with(o.pit.all, plot(all.cov~TSDL.cov, pch='+', main='Original SDL'))
with(o.pit.all, plot(all.cov~x1_per_line.cov, pch='+', main='Original 1 per line'))
with(r.pit.all, plot(all.cov~TSDL.cov, pch='+', main='Randoop SDl'))
with(r.pit.all, plot(all.cov~x1_per_line.cov, pch='+', main='Randoop 1 per line'))
@
\label{fig:sdl1}
\caption{Statement Deletion vs One per line.}
\end{figure*}
<<results='asis'>>=
o <- subset(stats.df, kind == 'original')
o.x <- subset(o, select=c('operator', 'r.square', 'mutant.reduction', 'sd.reduction', 'sample', 'score.multiplier'))
colnames(o.x) <- c('operator', 'r^2', 'm.red', 'm.sd', 'num', 'B*')
print(xtable(o.x, caption='Original', label='table:try1'), include.rownames=FALSE)
# , sanitize.text.function = function(x){x}, include.rownames=FALSE)
@
<<results='asis'>>=
r <- subset(stats.df, kind == 'randoop')
r.x <- subset(r, select=c('operator','r.square', 'mutant.reduction', 'sd.reduction', 'sample', 'score.multiplier'))
colnames(r.x) <- c('operator', 'r^2', 'm.red','m.sd', 'num', 'B*')
print(xtable(r.x, caption='Randoop', label='table:try1'), include.rownames=FALSE)
@


\subsection{Correlation Analysis}

$ \mu\{ M_{all}| M_{sample} \} = 0 + \beta_1 \times M{sample}  $

The interesting part is (1) the $\beta_1$ coefficient which tells how much we must multiply the  sampled/selected score to get original score, and $R^2$ which tells us how close the score is.

\subsubsection{sdl.mut.avg and mut.avg}

% <<>>=
% c <- cor(subset(m, select=c(sdl.mut.avg, mut.avg)) , method="kendall", use="pairwise")['sdl.mut.avg', 'mut.avg']
% d <- cor(subset(m, select=c(lrnd.mut.avg, mut.avg)) , method="kendall", use="pairwise")['lrnd.mut.avg', 'mut.avg']
% @

$ \mu_\{M_{all} | M_{sdl}\} = Sexpr{sdl.est} \times M_{sdl} \text{ : } R^2 = Sexpr{sdl.r} \text{,} \tau_{\beta} = Sexpr{c}$

\subsubsection{lrnd.mut.avg and mut.avg}

$ \mu_\{M_{all} | M_{rnd}\} = Sexpr{rnd.est} \times M_{rnd} \text{ : } R^2 = Sexpr{rnd.r} \text{,} \tau_{\beta} = Sexpr{d}$

\subsection{Regression Analysis}
The next question we will tackle is if there exist a difference at all between SDL mutagen and the random sampling. We can perform two statistical tests to answer this question. For the first, we run a simple paired t-test.

%<<>>=
%s <- t.test(lrnd.mut.avg, sdl.mut.avg, paired=T)
%@

The paired t-test failed to sufficient difference between the two distributions with a two-sided p-value of Sexpr{s.p.value}. That is, we do not have sufficient evidence to conclude that the two distributions are different.

The second approch is convoluted (I am a little unclear here.). We try to predict the original mutation score using the mutation score from both sdl and random sampling.
Our full model is

$ \mu_\{M_{all} | M_{sdl}, M_{rnd}\} = 0 + \beta_1 {sdl} + \beta_2 M_{rnd} + \beta_3 M_{sdl} M_{rnd}  $
\begin{small}
% <<results='asis'>>=
% s <- subset(m, select=c(mut.avg,sdl.mut.avg))
% s$type <- 'sdl'
% colnames(s) <- c('mut', 'sample', 'type')
% r <- subset(m, select=c(mut.avg,lrnd.mut.avg))
% r$type <- 'rnd'
% colnames(r) <- c('mut', 'sample', 'type')
% all <- rbind(s, r)
% fm <- with(all, lm(mut ~ sample + type + sample:type))
% sm <- anova(fm)
% print(xtable(sm,caption="My caption"), size="\\small")
% @
\end{small}

We see that both type and sample:type are non-significant for predicting the final model, and sequentially removing them does not change the effectiveness. Hence, the techniques random sampling and statement deletion operator do not have any difference.

\begin{figure*}[t]
% <<tplots, fig.width=5, fig.height=5, out.width='.45\\linewidth', out.heigth='4in', echo=F, message=F, warn=F,fig.lp="fig:">>=
% t_f <- sdl.time~lrnd.time
% plot(t_f, main='SDL ~ RND',pch='*')
% abline(lm(t_f), col='red')
% detach(m)
% @
\caption{Time}
\label{fig:caddrm}
\end{figure*}

\begin{figure*}[t]
\includegraphics[totalheight=6cm]{fig/original.pdf}
\caption{Subsumption Original}
\label{fig:subsumptiono}
\end{figure*}[t]

\begin{figure*}[t]
\includegraphics[totalheight=6cm]{fig/randoop.pdf}
\caption{Subsumption Randoop}
\label{fig:subsumptionr}
\end{figure*}[t]

<<results, child='results.Rnw'>>=
@

\begin{enumerate}
\item [x] For op-selection to work, the distributions of mutations across different projects has to be similar enough. This can be checked by t-test.
\item [TODO] We should check if the distribution significantly different too.
\item [DONE] Use sampling
\item [DONE] Check on randoop samples too
\item [NO] Implement bomb (athrow) and see its mutation score, and see how close it comes.
\item [DONE] Compare random selection with class, method, and mutagen sampling.
\item [TODO] Report time for each, as cost.
\item [TODOX] Find the mutant count to LOC multiplier, and use it to get the probability of choosing for each project.
\end{enumerate}
\section{Results}

We see from the data that random sampling has a higher $R^2$, and also approximates the all  mutations score better.
However, caveat emptor. There is a difference between the number of mutators for SDL and RAND. If our approximation was right, we should have gotten nearly same number of mutators. However
The number of mutators in RND is
 Sexpr{sum(m.lrnd.pit.mutation.total) / sum(m.sdl.pit.mutation.total)} times SDL.

The total mutants for all is Sexpr{sum(m.pit.mutation.total)}, for rnd is Sexpr{sum(m.lrnd.pit.mutation.total)} and for sdl is Sexpr{sum(m.sdl.pit.mutation.total)}

\begin{enumerate}
\item [x] For op-selection to work, the distributions of mutations across different projects has to be similar enough. This can be checked by t-test.
\item [TODO] We should check if the distribution significantly different too.
\item [DONE] Use sampling
\item [DONE] Check on randoop samples too
\item [NO] Implement bomb (athrow) and see its mutation score, and see how close it comes.
\item [DONE] Compare random selection with class, method, and mutagen sampling.
\item [TODO] Report time for each, as cost.
\item [TODOX] Find the mutant count to LOC multiplier, and use it to get the probability of choosing for each project.
\end{enumerate}
