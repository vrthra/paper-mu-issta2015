\newcommand*\per{\scalebox{.5}{\%}}
\newcommand\Mua{Mutation analysis~}
\newcommand\mua{mutation analysis~}
\begin{comment}
<<setup, include=FALSE, cache=FALSE, message=FALSE, echo=FALSE>>=
#options(warn=-1)
options(digits=3)
opts_chunk$set(fig.path='figure/twocolumn-', fig.align='center', fig.show='hold', echo=FALSE)
render_listings()
logit<-function(x) log(x/(1-x))
@
<<chunk-hook, echo=2>>=
<<setup>>
@
\end{comment}


<<a,message=F, echo=F>>=
require(xtable)
require(data.table)
load('data/mu.rda')
@

% 10 pages for the main text + 1 for references.
\section{Introduction}
% 1. Describe the problem
% 2. State your contributions

Research in software testing often involves comparison between different testing techniques.
\Mua originally proposed by Lipton \cite{lipton1971fault} involves systematic transformation of program through introduction of first order syntactical changes.
The ability of test suites to distinguish the variants -- mutation score -- thus produced is taken as the effectiveness of the test suite\cite{ammann2008introduction}, and is used as an indicator for the effectiveness of the test suite against real faults\cite{andrews2005mutation}.

\Mua relies on two main assumptions: competant programmer hypothesis, and coupling effect\cite{budd1979mutation}.
Competant programmer hypothesis suggests that the version of program produced by a competant programmar is close to the final version,
while coupling effect claims that a test suite capable of catching first order bugs will also detect higher order bugs that contain these first order bugs.

For \mua to be used as a successful tool for comparison of testing techniques, the mutants produced by \mua should be similar in charecteristics to the real faults found in software.
This was investigated by Andrews et al.\cite{andrews2005mutation}, where it was found in one of the programs under consideration `space.c' for which real faults were available, the ease of detecting mutants was very close to the ease of detecting real faults.
However, for the other programs under consideration from siemens suite, the faults seeded by hand were much more harder to detect than mutants.
From this, Andrews et al. concluded that mutants produced were similar in characteristic to the real faults in terms of ease of detection.
However, this has come under attack for various reasons.
For example, the analysis depended on a single program which may not be a good representative of the other programs.
Further, Namin et al.\cite{namin2011theuse} found that different factors such as different programming languages, test suite size, and mutation operators chosen has an impact on whether the mutants produced are similar to real faults in terms of ease of detection.

This has led us to investigate real faults in software in a larger scale, especially paying attention to programming langauges of four different kinds: C, Java, Python and Haskell.
We have used the revision control histories of open source projects as obtained from Github\cite{github}. From these, we have identified changes that correspond to bugfixes using machine learning techniques. We have also identified the changes that are simple.

This research makes the following contributions.

\begin{enumerate}
\item contrib
\end{enumerate}

\section{Related Work}
Related work.
\section{Methodology}
Our methodology

\section{Analysis}

<<prepare,echo=F>>=
attach(m)
sdl_f <- mut.avg~mut.avg.sdl + 0
rnd_f <- mut.avg~mut.avg.rnd + 0

sdl_all <- lm(sdl_f)
rnd_all <- lm(rnd_f)

sdl_s <- summary(sdl_all)
rnd_s <- summary(rnd_all)

avg.est <- sdl_s$coefficients['mut.avg.sdl','Estimate']
rnd.est <- rnd_s$coefficients['mut.avg.rnd','Estimate']
@

\begin{figure*}[t]
<<plots, fig.width=5, fig.height=5, out.width='.45\\linewidth', out.heigth='4in', echo=F, message=F, warn=F,fig.lp="fig:">>=
plot(sdl_f, main='SDL ~ ALL',pch='*')
abline(sdl_all, col='red')
plot(rnd_f, main='RND ~ ALL',pch='*')
abline(rnd_all, col='blue')
abline(sdl_all, col='red', lty=2)
@
\caption{Plots}
\label{fig:caddrm}
\end{figure*}

\subsection{Correlation Analysis}

$ \mu_{all} = 0 + \beta_0 \times \mu_{sample}  $

The interesting part is (1) the $\beta_0$ coeff which tells how much we must multiply the  sampled/selected score to get original score, and $R^2$ which tells us how close the score   is.

\subsubsection{mut.avg.sdl and mut.avg}

$ \mu_{all} = 0 + \Sexpr{avg.est} \times \mu_{sdl}  $

\subsubsection{mut.avg.rnd and mut.avg}

$ \mu_{all} = 0 + \Sexpr{rnd.est} \times \mu_{rnd}  $

\section{Results}

We see from the data that random sampling has a higher $R^2$, and also approximates the all  mutations score better.
However, caveat emptor. There is a difference between the number of mutators for SDL and RAND. If our approximation was right, we should have gotten nearly same number of mutators. However
The number of mutators in RND is \Sexpr{sum(m$pit.mutation.total.rnd) / sum(m$pit.mutation.total.sdl)} times SDL.
