\section{Discussion}
\label{sec:discussion}

From our data, we see that for original and generated test suites, the
mutant reduction ratio and standard deviation was similar for stable strategies
(where the standard deviation of the reduction achieved was less than half
of the reduction achieved). This suggests that our analysis is equally
applicable for both generated test suites and original test suites.

Analysing the scores achieved further, we note that the accuracy of prediction
($R^2$) achieved by sampling is generally lower for generated test
suites. Especially, as the fraction of sampling becomes larger the accuracy
reduces drastically for generated test suites while the accuracy still
holds good for original test suites
Secondly, we also observe that the multiplier factor $\beta_1$ 
is smaller for generated test suites than original test suites. This
observation holds good for all sampling and operator selection strategies
examined.  Both these factors together suggest that generated
test suites have a higher variability in their sampled mutation scores than
original test suites. We note that since $\beta_1$ was generally smaller
for generated test suites than original test suites, the full mutation score
corresponding to a sampled mutation score for generated test suites would
be lower than what would be expected from original test suite.
Secondly, the amount of reduction that can be gained by sampling satisfying
a given accuracy is lower for generated test suites in comparison with original
test suites.

Considering different sampling strategies, we see a few general patterns. The
first is that sampling strategy can predict the final score with very
minimal loss in accuracy up to a fraction of $\frac{1}{64}$ of the total
number of mutants (Our experiments suggest up to $70$ times reduction,
with a low standard deviation of $6.72$). We did not cut off with the 99\%
rule as mentioned by Zhang et al.~\cite{zhang2013ase} (Originally proposed
by Offutt et al.~\cite{offutt1996experimental} for a different measurement),
and instead chose to observe that we obtained $R^2 > 0.96$ which provides
a high confidence in the accuracy of prediction.

The second pattern is that fractional sampling based on projects appears
to be better than that based on classes, which in turn is better than
methods and lines, with regard to the stability of the result (i.e. low
standard deviation).  This seems not very visible at lower levels (where
the reduction fraction is less than 32), but becomes more pronounced as the
mutant reduction fraction increases.  There seem to exist a hierarchy of
sampling, where by as sampling becomes more fine grained stability decreases.
We did not see the pattern found by Zhang et al.~\cite{zhang2013ase} where
method-based sampling had the highest accuracy. In fact, in our measurements,
the opposite may be true, with project based sampling getting the highest
correlation with the final score a majority of the time.

The third pattern is that the simplest sampling scheme --- simple random sampling ---
performs best compared to sampling schemes that tried to provide higher weight
to specific elements based on specific criteria. More importantly, operator selection
seems to be generally worse off in predicting the final score than any of the random
selection schemes.

