\section{Discussion}
\label{sec:discussion}

% About Randoop and Original
From our data, we see that for original and generated test suites, the
mutant reduction ratio and standard deviation were similar for stable strategies
(where the standard deviation of the reduction achieved was less than half
of the reduction achieved). This suggests that our analysis is equally
applicable for both original test suites and generated ones.

% About Randoop and Original
Analyzing the scores further, we note that the accuracy of prediction
($R^2$) achieved by sampling is generally lower for generated test
suites. As the sampling denominator increases, the correlation
reduces drastically for generated test suites while correlation remains strong
for original test suites
Secondly, we also observe that the multiplier factor $\beta_1$
is smaller for generated test suites than original test suites. This
observation holds good for all sampling and operator selection strategies
examined.  Both these factors together suggest that generated
test suites have a higher variability in their sampled mutation scores than
original test suites. We note that since $\beta_1$ was generally smaller
for generated test suites than original test suites, the full mutation score
corresponding to a sampled mutation score for generated test suites would
be lower than what would be expected from the original test suite.
Secondly, the amount of reduction that can be gained by sampling that must reach a given correlation is lower for generated test suites.

% Different sampling strategies
Considering different sampling strategies, based on their stability and mutation
reduction achieved, we note that using simple sampling (\textit{sample(x)})
produced the most stable results. For both generated and original test suites,
this strategy produced valid results for all different scopes and sampling
ratios. It also was able to achieve the highest mutation reduction of $72.63$,
that is only $1.37\%$ of the original mutants were needed to produce a highly
correlated ($R^2 = 0.97$) mutation score, with minimal deviation
($\sigma = 6.36$) for original test suites. While the correlation is weaker for
generated test suites it is still quite strong, with $R^2 = 0.90$

Our \textit{round(x)}, the strategy of ignoring low complexity elements,
did not pay off, with line scope having a tendency to become unstable and
inaccurate at low sampling ratios. At very low sampling ratio ($1/64$), the
strategy with method scope also becomes unstable and inaccurate. On the whole
\textit{round(x)} did not perform as well as \textit{sample(x)}

The strategy of \textit{ceil(x)} also did not perform well. While the
results were generally stable, the reduction ratios achieved were lower than
\textit{sample(x)}, with similar correlations.

For line count strategies, only the external sampling produced
consistent results, which were again not as good as \textit{round(x)}
for similar reduction ratios. Secondly, we had hypothesized
that coverage was a significant contributor to having a high
correlation with the full mutation score. However we could find no
evidence in support of this hypothesis with line, method, class, and
project scope showing similar correlation and consistency at similar
reduction ratios.

The one per element strategy did not have stable results in general except
for at the scope of line (at which point it was same as the count-per-element
strategy).

While the x\% per operator strategy produced consistent results, its
consistency was not as good as \textit{sample(x)}, and was either on
par with or worse than \textit{sample(x)} in $R^2$.

Considering mutation operators, only three operators \textit{NC},
\textit{RC},and \textit{DC} had consistent results (and only for
original test suites). The n-selection strategy had high correlation and
consistency (though low reduction ratio) until 7-selection (removing
the seven most numerous operators) for both generated and original
test suites.  Javalanche did not have a high mutation reduction ratio,
though it had high $R^2$ for both kinds of test suites. The other
operator selection methods performed poorly on generated test suites,
while they had high correlation and reasonable stability on original test
suites. However, for comparable mutation reduction levels, they all performed
worse than random sampling strategies.

Considering different sampling strategies, we see a few general patterns. The
first is that sampling strategy can predict the final score with very
minimal loss up to a fraction of $\frac{1}{64}$ of the total
number of mutants (our experiments suggest up to $70$ times reduction,
with a low standard deviation of $6.72$). We did not cut off with the 99\%
rule as mentioned by Zhang et al.~\cite{zhang2013ase} (originally proposed
by Offutt et al.~\cite{offutt1996experimental} for a different measurement),
and instead chose to observe that we obtained $R^2 > 0.96$ which provides
a high confidence in the accuracy of prediction.

The second pattern is that fractional sampling based on projects
appears to be better than that based on classes, which in turn is
better than methods and lines, with regard to the stability of the
result (i.e. low standard deviation).  This is not clear at lower
levels of reduction ($< 1/32$), but becomes more pronounced for lower
ratios. There seems to exist a hierarchy of sampling, whereby as sampling becomes
more fine grained stability decreases.  We did not observe the pattern
found by Zhang et al.~\cite{zhang2013ase} where method-based sampling
had the best correlations. In fact, in our measurements, the opposite
may be true, with project based sampling having the highest
$R^2$ a majority of the time.

The third "pattern" is that the simplest sampling scheme --- simple
random sampling --- performs best compared to sampling schemes that
tried to provide higher weight to specific elements based on specific
criteria. Additionally, operator selection seems to be generally worse
off in predicting the final mutation score than any of the random selection
schemes.  In principle, if we could devise sampling methods that
somehow captured only the most discriminative mutants for programs,
they might perform better than any of these; our results suggest that
most schemes that (we assume) aim at this goal do not in fact achieve
the goal over a large body of programs, and so tend to perform worse
than simply randomly sampling the whole pool of mutants, which has
attractive simplicity in the absence of a better strategy.

