<<t.tests,child='src/ttests.Rtex'>>=
@
<<tblsingles, child='src/tables.Rtex'>>=
@
\section{Analysis}
\label{sec:analysis}
For the purpose of comparing different mutation reduction strategies, we
computed the average \textit{test utility}, and \textit{assert utility}
of different strategies.

We use the mutant reduction strategy to select a subset of mutants from the
original set. We then collect all test cases that were able to kill any of
the selected set of mutants. Next we compute the non-redundant test cases
by discarding any test case that can be removed without letting any of the
mutants escape. This is repeated and averaged over multiple runs, and the
average number of non-redundant test cases are found.

While computing the test cases, we also collect assertion information each
test case has. We compute this as the number of asserts in the test case
incremented by one to account for test cases without any asserts. Thus we
compute both the assertions from non-redundant test cases for the mutant
selection, and also the assertions for the full set of killing test cases
for the mutant selection.

Let $M$ and $M_{strategy}$ denote the original set of mutants and the
reduced set of mutants respectively. Test suite composed of test cases
that are able to kill an $m$ where  $ m \in M_{strategy}$ is given by
$T_{strategy}$, and the total number of \texttt{assert}s in $T_{strategy}$
is given by $asserts(T_{strategy})$. We find a minimal test suite that is
capable of killing all mutants under $M_{strategy}$ by a greedy algorithm
of iteratively choosing the test cases with the largest amount of kills
for remaining mutants, and is given by $min(T_{strategy})$. Similarly, for
baseline, we build a set of randomly selected mutants $M_{random}$, such that
$|M_{random}| = |M_{strategy}|$. Given any set of tests $T$, the total number
of mutants that can be killed by the test cases in $T$ is given by $k(T)$,
and those mutants that are covered by the test cases are given by $c(T)$.

The \textit{test utility} approximates the extra tests a selection strategy
requires comparing a random sampling that suggest the same number of mutants.
We  compute it as the difference between the number of non-redundant test
cases the mutant selection achieved and the number of non-redundant test
cases of a random sample of same number of mutants.  The result is reported
as a percentage of the non-redundant tests of the random sample which is
taken as the baseline. That is, test utility is given by
\[\left(\frac{|min(T_{strategy})|}{|min(T_{random})|}-1\right)\times 100.0\]

Positive value test utility denotes that the strategy requires more tests
than random selection, and negative test utility denotes that the strategy
needs less test cases than random.  Values close to zero basically denote
that the strategy roughly suggest the same number of tests. Note that the
comparison here is between the size of tests and it does not imply any subset
relationship between test suites.

Similarly \textit{assert utility} of a strategy is computed as the difference
between the number of assertions of non-redundant test cases the mutant
selection achieved and the number of assertions the test cases for random
sample of same number of mutants. As before, it is reported as a percentage
of the asserts of the non-redundant tests of the random sample which is
taken as the baseline. It is given by
\[\left(\frac{|asserts(min(T_{strategy}))|}{|asserts(min(T_{random}))|}-1\right)\times 100.0\]

\textit{Baseline effectiveness} is computed by getting the number
of mutants selected by the strategy, and selecting the same number of mutants
randomly, collecting the test cases that kill any of these mutants, and
applying the same test cases against the original set of mutants. The result
is then divided by the original number of detected mutants. It is given by
\[\frac{|k(T_{random})|}{|k(M)|}\times 100.0\]

\textit{Strategy effectiveness}~\footnote{Also called mutation score by Mresa
et al.~\cite{mresa1999efficiency}} is computed by collecting all the test
cases that detected any of the mutants in the selected subset, and applying
these test cases to the complete set of mutants. The mutation score thus
obtained is then divided by the original number of detected mutants.  This is
reported as difference from the corresponding baseline effectiveness, given by
\[\frac{|k(T_{strategy})| - |k(T_{random})|}{|k(M)|}\times100.0\]


\begin{figure*}
<<mutantdet, child='src/fig.mutantdist.Rtex'>>=
@
\caption{The relative contribution of mutation operators in terms of fraction of mutants produced and detected.}
\label{fig:mutationshare}
\end{figure*}

\begin{figure*}
<<testdet, child='src/fig.testdist.Rtex'>>=
@
\caption{The relative contribution of mutation operators in terms of fraction of tests killed and covered.}
\label{fig:mutationkills}
\end{figure*}

\subsection{Statistical tests}
To compute the actual advantage gained by each of the different strategies, we
calculated the paired t-test between the number of test cases reported by the
strategy, and that of random selection of the same number of operators as
baseline, that is 
\[T.test\left(|T_{strategy}|\sim{}|T_{random}|\right)\]

We also calculated the paired t-test between the number of mutants from the
full set which were killed by the test cases killing any of the mutants from
the selected mutants by mutation reduction strategy. That is
\[T.test\left(k(T_{strategy})\sim{}k(T_{random})\right)\]

<<tblttest, child='src/tbl.ttests.Rtex'>>=
@
